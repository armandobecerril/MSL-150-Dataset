{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb8ebf8",
   "metadata": {},
   "source": [
    "Notebook 01 ‚Äì An√°lisis Exploratorio del Dataset MSL-150\n",
    "üìå 1. Introducci√≥n\n",
    "\n",
    "Este notebook realiza un an√°lisis exploratorio inicial (EDA) del dataset\n",
    "MSL-150_Mexican_Sign_Language_Dataset.csv, disponible en Zenodo.\n",
    "\n",
    "El objetivo principal es:\n",
    "\n",
    "Verificar la estructura del dataset\n",
    "\n",
    "Confirmar la presencia de las 150 clases\n",
    "\n",
    "Identificar n√∫mero de muestras por clase\n",
    "\n",
    "Validar coherencia interna en los datos\n",
    "\n",
    "Explorar distribuci√≥n, filas, columnas y primeros registros\n",
    "\n",
    "Este an√°lisis es la base para los notebooks posteriores (preparaci√≥n de datos, entrenamiento y evaluaci√≥n de modelos secuenciales).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e31df9b",
   "metadata": {},
   "source": [
    "Notebook 01 ‚Äì Exploratory Data Analysis (EDA) for MSL-150 Dataset\n",
    "üìå 1. Introduction\n",
    "\n",
    "This notebook performs an initial exploratory data analysis of the\n",
    "MSL-150_Mexican_Sign_Language_Dataset.csv, available on Zenodo.\n",
    "\n",
    "Objectives:\n",
    "\n",
    "Inspect dataset structure\n",
    "\n",
    "Confirm the 150 expected classes\n",
    "\n",
    "Count samples per class\n",
    "\n",
    "Validate internal consistency\n",
    "\n",
    "Explore columns, sample rows, and key statistics\n",
    "\n",
    "This EDA supports the next notebooks on data preparation and sequential model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b62b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librer√≠as cargadas correctamente.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"Librer√≠as cargadas correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d053e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terms: 150\n"
     ]
    }
   ],
   "source": [
    "# Ruta base del repositorio\n",
    "BASE_REPO = \"/Users/armandobecerril/PhD/MSL-150\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Datos crudos / CSV maestro\n",
    "# -----------------------------\n",
    "# Aqu√≠ tienes el CSV grande y el zip con todos los npy tal como vienen de Zenodo\n",
    "base_dir = f\"{BASE_REPO}/data/raw\"          # equivale a LSM_DATA\n",
    "base_dir_csv = base_dir                      # para mantener compatibilidad con c√≥digo viejo\n",
    "\n",
    "# Si quieres referirte directamente al CSV:\n",
    "master_csv_path = f\"{base_dir}/MSL-150_Mexican_Sign_Language_Dataset.csv\"\n",
    "\n",
    "# -----------------------------\n",
    "# 2) CSV estandarizados (si los generas de nuevo)\n",
    "# -----------------------------\n",
    "std_dir = f\"{BASE_REPO}/data/standardized\"   # equivale a LSM_DATA_01_STD_CSV\n",
    "# (la carpeta se crea cuando corras el pipeline que los genere)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) NPY completos y subset demo\n",
    "# -----------------------------\n",
    "np_dir_csv     = f\"{BASE_REPO}/data/raw_npy\"     # equivale a LSM_DATA_03_PREP_NP (todos los .npy)\n",
    "sample_npy_dir = f\"{BASE_REPO}/data/sample_npy\"  # subset de 5 palabras para pruebas r√°pidas\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Modelos entrenados (todas las versiones)\n",
    "# -----------------------------\n",
    "models_versions = f\"{BASE_REPO}/models/trained_models\"  # equivale a LSM_DATA_04_MODEL_VERSIONS\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Diccionario de t√©rminos\n",
    "# -----------------------------\n",
    "terms_path = f\"{BASE_REPO}/data/dictionary/terms.txt\"\n",
    "\n",
    "# Construcci√≥n del diccionario de vocabulario (igual que antes)\n",
    "frames = 30\n",
    "samples = 800          # o 200 si quieres que refleje el subset\n",
    "\n",
    "\n",
    "videos_dict_aug = {}\n",
    "with open(terms_path, 'r') as file:\n",
    "    for line in file:\n",
    "        term = line.strip()\n",
    "        videos_dict_aug[term] = samples\n",
    "\n",
    "print(f\"Total terms: {len(videos_dict_aug)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16bd770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VIDEO_SAMPLE', 'CLASSIFICATION', 'FRAME', 'TIMESTAMP', 'RIGHT_WRIST_X', 'RIGHT_WRIST_Y', 'RIGHT_WRIST_Z', 'RIGHT_THUMB_CMC_X', 'RIGHT_THUMB_CMC_Y', 'RIGHT_THUMB_CMC_Z', 'RIGHT_THUMB_MCP_X', 'RIGHT_THUMB_MCP_Y', 'RIGHT_THUMB_MCP_Z', 'RIGHT_THUMB_IP_X', 'RIGHT_THUMB_IP_Y', 'RIGHT_THUMB_IP_Z', 'RIGHT_THUMB_TIP_X', 'RIGHT_THUMB_TIP_Y', 'RIGHT_THUMB_TIP_Z', 'RIGHT_INDEX_FINGER_MCP_X', 'RIGHT_INDEX_FINGER_MCP_Y', 'RIGHT_INDEX_FINGER_MCP_Z', 'RIGHT_INDEX_FINGER_PIP_X', 'RIGHT_INDEX_FINGER_PIP_Y', 'RIGHT_INDEX_FINGER_PIP_Z', 'RIGHT_INDEX_FINGER_DIP_X', 'RIGHT_INDEX_FINGER_DIP_Y', 'RIGHT_INDEX_FINGER_DIP_Z', 'RIGHT_INDEX_FINGER_TIP_X', 'RIGHT_INDEX_FINGER_TIP_Y', 'RIGHT_INDEX_FINGER_TIP_Z', 'RIGHT_MIDDLE_FINGER_MCP_X', 'RIGHT_MIDDLE_FINGER_MCP_Y', 'RIGHT_MIDDLE_FINGER_MCP_Z', 'RIGHT_MIDDLE_FINGER_PIP_X', 'RIGHT_MIDDLE_FINGER_PIP_Y', 'RIGHT_MIDDLE_FINGER_PIP_Z', 'RIGHT_MIDDLE_FINGER_DIP_X', 'RIGHT_MIDDLE_FINGER_DIP_Y', 'RIGHT_MIDDLE_FINGER_DIP_Z', 'RIGHT_MIDDLE_FINGER_TIP_X', 'RIGHT_MIDDLE_FINGER_TIP_Y', 'RIGHT_MIDDLE_FINGER_TIP_Z', 'RIGHT_RING_FINGER_MCP_X', 'RIGHT_RING_FINGER_MCP_Y', 'RIGHT_RING_FINGER_MCP_Z', 'RIGHT_RING_FINGER_PIP_X', 'RIGHT_RING_FINGER_PIP_Y', 'RIGHT_RING_FINGER_PIP_Z', 'RIGHT_RING_FINGER_DIP_X', 'RIGHT_RING_FINGER_DIP_Y', 'RIGHT_RING_FINGER_DIP_Z', 'RIGHT_RING_FINGER_TIP_X', 'RIGHT_RING_FINGER_TIP_Y', 'RIGHT_RING_FINGER_TIP_Z', 'RIGHT_PINKY_MCP_X', 'RIGHT_PINKY_MCP_Y', 'RIGHT_PINKY_MCP_Z', 'RIGHT_PINKY_PIP_X', 'RIGHT_PINKY_PIP_Y', 'RIGHT_PINKY_PIP_Z', 'RIGHT_PINKY_DIP_X', 'RIGHT_PINKY_DIP_Y', 'RIGHT_PINKY_DIP_Z', 'RIGHT_PINKY_TIP_X', 'RIGHT_PINKY_TIP_Y', 'RIGHT_PINKY_TIP_Z', 'LEFT_WRIST_X', 'LEFT_WRIST_Y', 'LEFT_WRIST_Z', 'LEFT_THUMB_CMC_X', 'LEFT_THUMB_CMC_Y', 'LEFT_THUMB_CMC_Z', 'LEFT_THUMB_MCP_X', 'LEFT_THUMB_MCP_Y', 'LEFT_THUMB_MCP_Z', 'LEFT_THUMB_IP_X', 'LEFT_THUMB_IP_Y', 'LEFT_THUMB_IP_Z', 'LEFT_THUMB_TIP_X', 'LEFT_THUMB_TIP_Y', 'LEFT_THUMB_TIP_Z', 'LEFT_INDEX_FINGER_MCP_X', 'LEFT_INDEX_FINGER_MCP_Y', 'LEFT_INDEX_FINGER_MCP_Z', 'LEFT_INDEX_FINGER_PIP_X', 'LEFT_INDEX_FINGER_PIP_Y', 'LEFT_INDEX_FINGER_PIP_Z', 'LEFT_INDEX_FINGER_DIP_X', 'LEFT_INDEX_FINGER_DIP_Y', 'LEFT_INDEX_FINGER_DIP_Z', 'LEFT_INDEX_FINGER_TIP_X', 'LEFT_INDEX_FINGER_TIP_Y', 'LEFT_INDEX_FINGER_TIP_Z', 'LEFT_MIDDLE_FINGER_MCP_X', 'LEFT_MIDDLE_FINGER_MCP_Y', 'LEFT_MIDDLE_FINGER_MCP_Z', 'LEFT_MIDDLE_FINGER_PIP_X', 'LEFT_MIDDLE_FINGER_PIP_Y', 'LEFT_MIDDLE_FINGER_PIP_Z', 'LEFT_MIDDLE_FINGER_DIP_X', 'LEFT_MIDDLE_FINGER_DIP_Y', 'LEFT_MIDDLE_FINGER_DIP_Z', 'LEFT_MIDDLE_FINGER_TIP_X', 'LEFT_MIDDLE_FINGER_TIP_Y', 'LEFT_MIDDLE_FINGER_TIP_Z', 'LEFT_RING_FINGER_MCP_X', 'LEFT_RING_FINGER_MCP_Y', 'LEFT_RING_FINGER_MCP_Z', 'LEFT_RING_FINGER_PIP_X', 'LEFT_RING_FINGER_PIP_Y', 'LEFT_RING_FINGER_PIP_Z', 'LEFT_RING_FINGER_DIP_X', 'LEFT_RING_FINGER_DIP_Y', 'LEFT_RING_FINGER_DIP_Z', 'LEFT_RING_FINGER_TIP_X', 'LEFT_RING_FINGER_TIP_Y', 'LEFT_RING_FINGER_TIP_Z', 'LEFT_PINKY_MCP_X', 'LEFT_PINKY_MCP_Y', 'LEFT_PINKY_MCP_Z', 'LEFT_PINKY_PIP_X', 'LEFT_PINKY_PIP_Y', 'LEFT_PINKY_PIP_Z', 'LEFT_PINKY_DIP_X', 'LEFT_PINKY_DIP_Y', 'LEFT_PINKY_DIP_Z', 'LEFT_PINKY_TIP_X', 'LEFT_PINKY_TIP_Y', 'LEFT_PINKY_TIP_Z', 'NOSE_X', 'NOSE_Y', 'NOSE_Z', 'NOSE_V', 'LEFT_EYE_INNER_X', 'LEFT_EYE_INNER_Y', 'LEFT_EYE_INNER_Z', 'LEFT_EYE_INNER_V', 'LEFT_EYE_X', 'LEFT_EYE_Y', 'LEFT_EYE_Z', 'LEFT_EYE_V', 'LEFT_EYE_OUTER_X', 'LEFT_EYE_OUTER_Y', 'LEFT_EYE_OUTER_Z', 'LEFT_EYE_OUTER_V', 'RIGHT_EYE_INNER_X', 'RIGHT_EYE_INNER_Y', 'RIGHT_EYE_INNER_Z', 'RIGHT_EYE_INNER_V', 'RIGHT_EYE_X', 'RIGHT_EYE_Y', 'RIGHT_EYE_Z', 'RIGHT_EYE_V', 'RIGHT_EYE_OUTER_X', 'RIGHT_EYE_OUTER_Y', 'RIGHT_EYE_OUTER_Z', 'RIGHT_EYE_OUTER_V', 'LEFT_EAR_X', 'LEFT_EAR_Y', 'LEFT_EAR_Z', 'LEFT_EAR_V', 'RIGHT_EAR_X', 'RIGHT_EAR_Y', 'RIGHT_EAR_Z', 'RIGHT_EAR_V', 'MOUTH_LEFT_X', 'MOUTH_LEFT_Y', 'MOUTH_LEFT_Z', 'MOUTH_LEFT_V', 'MOUTH_RIGHT_X', 'MOUTH_RIGHT_Y', 'MOUTH_RIGHT_Z', 'MOUTH_RIGHT_V', 'LEFT_SHOULDER_X', 'LEFT_SHOULDER_Y', 'LEFT_SHOULDER_Z', 'LEFT_SHOULDER_V', 'RIGHT_SHOULDER_X', 'RIGHT_SHOULDER_Y', 'RIGHT_SHOULDER_Z', 'RIGHT_SHOULDER_V', 'LEFT_ELBOW_X', 'LEFT_ELBOW_Y', 'LEFT_ELBOW_Z', 'LEFT_ELBOW_V', 'RIGHT_ELBOW_X', 'RIGHT_ELBOW_Y', 'RIGHT_ELBOW_Z', 'RIGHT_ELBOW_V', 'LEFT_WRIST_X.1', 'LEFT_WRIST_Y.1', 'LEFT_WRIST_Z.1', 'LEFT_WRIST_V', 'RIGHT_WRIST_X.1', 'RIGHT_WRIST_Y.1', 'RIGHT_WRIST_Z.1', 'RIGHT_WRIST_V', 'LEFT_PINKY_X', 'LEFT_PINKY_Y', 'LEFT_PINKY_Z', 'LEFT_PINKY_V', 'RIGHT_PINKY_X', 'RIGHT_PINKY_Y', 'RIGHT_PINKY_Z', 'RIGHT_PINKY_V', 'LEFT_INDEX_X', 'LEFT_INDEX_Y', 'LEFT_INDEX_Z', 'LEFT_INDEX_V', 'RIGHT_INDEX_X', 'RIGHT_INDEX_Y', 'RIGHT_INDEX_Z', 'RIGHT_INDEX_V', 'LEFT_THUMB_X', 'LEFT_THUMB_Y', 'LEFT_THUMB_Z', 'LEFT_THUMB_V', 'RIGHT_THUMB_X', 'RIGHT_THUMB_Y', 'RIGHT_THUMB_Z', 'RIGHT_THUMB_V', 'LEFT_HIP_X', 'LEFT_HIP_Y', 'LEFT_HIP_Z', 'LEFT_HIP_V', 'RIGHT_HIP_X', 'RIGHT_HIP_Y', 'RIGHT_HIP_Z', 'RIGHT_HIP_V']\n"
     ]
    }
   ],
   "source": [
    "df_head = pd.read_csv(master_csv_path, nrows=5)\n",
    "print(df_head.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f6d7489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hospital</th>\n",
       "      <td>24000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terapia</th>\n",
       "      <td>24000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paciente</th>\n",
       "      <td>24000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfermero</th>\n",
       "      <td>24000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfermera</th>\n",
       "      <td>24000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctor</th>\n",
       "      <td>24000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aborto</th>\n",
       "      <td>24000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virus</th>\n",
       "      <td>24000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jarabe</th>\n",
       "      <td>24000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caminar</th>\n",
       "      <td>24000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           num_samples\n",
       "hospital         24000\n",
       "terapia          24000\n",
       "paciente         24000\n",
       "enfermero        24000\n",
       "enfermera        24000\n",
       "doctor           24000\n",
       "aborto           24000\n",
       "virus            24000\n",
       "jarabe           24000\n",
       "caminar          24000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de clases        : 150\n",
      "M√≠nimo muestras/clase  : 8676\n",
      "M√°ximo muestras/clase  : 24000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "label_col = \"CLASSIFICATION\"\n",
    "\n",
    "class_counts = {}\n",
    "chunksize = 50_000\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    master_csv_path,\n",
    "    chunksize=chunksize,\n",
    "    usecols=[label_col],\n",
    "    dtype={label_col: str}  # üëà forzamos a string\n",
    "):\n",
    "    # Normalizamos en cada chunk\n",
    "    chunk[label_col] = (\n",
    "        chunk[label_col]\n",
    "        .fillna(\"NaN_VALUE\")  # si quieres puedes luego filtrar esto\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    vc = chunk[label_col].value_counts()\n",
    "    for label, cnt in vc.items():\n",
    "        class_counts[label] = class_counts.get(label, 0) + cnt\n",
    "\n",
    "class_counts_df = (\n",
    "    pd.DataFrame.from_dict(class_counts, orient=\"index\", columns=[\"num_samples\"])\n",
    "      .sort_values(\"num_samples\", ascending=False)\n",
    ")\n",
    "\n",
    "display(class_counts_df.head(10))\n",
    "print(f\"\\nTotal de clases        : {class_counts_df.shape[0]}\")\n",
    "print(f\"M√≠nimo muestras/clase  : {class_counts_df['num_samples'].min()}\")\n",
    "print(f\"M√°ximo muestras/clase  : {class_counts_df['num_samples'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7e7a35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de clases: 150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '10',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " 'abeja',\n",
       " 'aborto',\n",
       " 'abril',\n",
       " 'accidente',\n",
       " 'agosto',\n",
       " 'ahora',\n",
       " 'ambulancia',\n",
       " 'analisis',\n",
       " 'ayer',\n",
       " 'beber',\n",
       " 'bien',\n",
       " 'boca',\n",
       " 'brazo',\n",
       " 'calentura',\n",
       " 'caliente',\n",
       " 'camaron',\n",
       " 'caminar',\n",
       " 'cancer',\n",
       " 'cansado',\n",
       " 'cintura',\n",
       " 'cita',\n",
       " 'cocinar',\n",
       " 'codo',\n",
       " 'comer',\n",
       " 'como',\n",
       " 'confundido',\n",
       " 'contagiar',\n",
       " 'convulciones',\n",
       " 'coronavirus',\n",
       " 'correr',\n",
       " 'cuantos',\n",
       " 'cuello',\n",
       " 'debil',\n",
       " 'descansar',\n",
       " 'diario',\n",
       " 'diarrea',\n",
       " 'diciembre',\n",
       " 'doctor',\n",
       " 'dolor',\n",
       " 'domingo',\n",
       " 'dormir',\n",
       " 'duda',\n",
       " 'duro',\n",
       " 'embarazo',\n",
       " 'emergencia',\n",
       " 'enero',\n",
       " 'enfermera',\n",
       " 'enfermero',\n",
       " 'enfermo',\n",
       " 'espalda',\n",
       " 'esposa',\n",
       " 'esposo',\n",
       " 'estresado',\n",
       " 'estudiar',\n",
       " 'farmacia',\n",
       " 'febrero',\n",
       " 'fractura',\n",
       " 'frio',\n",
       " 'garganta',\n",
       " 'gases',\n",
       " 'gato',\n",
       " 'gripa',\n",
       " 'hija',\n",
       " 'hijo',\n",
       " 'hombro',\n",
       " 'hospital',\n",
       " 'hoy',\n",
       " 'huesos',\n",
       " 'infarto',\n",
       " 'infeccion',\n",
       " 'inflamacion',\n",
       " 'interpretar',\n",
       " 'inyeccion',\n",
       " 'ir',\n",
       " 'jarabe',\n",
       " 'jueves',\n",
       " 'julio',\n",
       " 'junio',\n",
       " 'lento',\n",
       " 'lesion',\n",
       " 'lunes',\n",
       " 'mal',\n",
       " 'mama',\n",
       " 'manana',\n",
       " 'mano',\n",
       " 'mareo',\n",
       " 'martes',\n",
       " 'marzo',\n",
       " 'mayo',\n",
       " 'mejor',\n",
       " 'miercoles',\n",
       " 'moco',\n",
       " 'muneca',\n",
       " 'nariz',\n",
       " 'no',\n",
       " 'no_ver',\n",
       " 'normal',\n",
       " 'nosotros',\n",
       " 'noviembre',\n",
       " 'nunca',\n",
       " 'octubre',\n",
       " 'ojo',\n",
       " 'oreja',\n",
       " 'orina',\n",
       " 'paciente',\n",
       " 'panza',\n",
       " 'papa',\n",
       " 'para_que',\n",
       " 'pastillas',\n",
       " 'pelear',\n",
       " 'pene',\n",
       " 'peor',\n",
       " 'perro',\n",
       " 'pie',\n",
       " 'piernas',\n",
       " 'pollo',\n",
       " 'popo',\n",
       " 'por_que',\n",
       " 'pregunta',\n",
       " 'rapido',\n",
       " 'recibir',\n",
       " 'revisar',\n",
       " 'rodilla',\n",
       " 'sabado',\n",
       " 'sangre',\n",
       " 'sed',\n",
       " 'septiembre',\n",
       " 'si',\n",
       " 'siempre',\n",
       " 'suave',\n",
       " 'terapia',\n",
       " 'tobillo',\n",
       " 'tos',\n",
       " 'trabajar',\n",
       " 'urgencia',\n",
       " 'vagina',\n",
       " 'viernes',\n",
       " 'virus',\n",
       " 'vomito',\n",
       " 'yo']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar SOLO CLASSIFICATION, convirtiendo todo a string\n",
    "labels = pd.read_csv(master_csv_path, usecols=['CLASSIFICATION'], dtype=str)\n",
    "\n",
    "# Reemplazar NaN por texto expl√≠cito\n",
    "labels['CLASSIFICATION'] = labels['CLASSIFICATION'].fillna(\"NaN_VALUE\")\n",
    "\n",
    "# Remover espacios alrededor\n",
    "labels['CLASSIFICATION'] = labels['CLASSIFICATION'].str.strip()\n",
    "\n",
    "# Obtener lista ordenada\n",
    "unique_labels = sorted(labels['CLASSIFICATION'].unique())\n",
    "\n",
    "print(\"N√∫mero de clases:\", len(unique_labels))\n",
    "unique_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4523019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49188fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
