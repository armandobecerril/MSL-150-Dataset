{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c135d2",
   "metadata": {},
   "source": [
    "# 00 – Data Tasks, Functions and Preparation for the MSL-150 Dataset\n",
    "\n",
    "This notebook documents the end-to-end data preparation pipeline used in this work,\n",
    "from the original MSL videos to the final NumPy tensors consumed by the recurrent\n",
    "neural networks.\n",
    "\n",
    "It is organized into the following sections:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f137bd7",
   "metadata": {},
   "source": [
    "# 0 – Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b4079a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[53290]: Class CaptureDelegate is implemented in both /Users/armandobecerril/anaconda3/envs/tensorflow/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x3135d65d8) and /Users/armandobecerril/anaconda3/envs/tensorflow/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x314748860). One of the two will be used. Which one is undefined.\n",
      "objc[53290]: Class CVWindow is implemented in both /Users/armandobecerril/anaconda3/envs/tensorflow/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x3135d6628) and /Users/armandobecerril/anaconda3/envs/tensorflow/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x16a0f0a68). One of the two will be used. Which one is undefined.\n",
      "objc[53290]: Class CVView is implemented in both /Users/armandobecerril/anaconda3/envs/tensorflow/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x3135d6650) and /Users/armandobecerril/anaconda3/envs/tensorflow/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x16a0f0a90). One of the two will be used. Which one is undefined.\n",
      "objc[53290]: Class CVSlider is implemented in both /Users/armandobecerril/anaconda3/envs/tensorflow/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x3135d6678) and /Users/armandobecerril/anaconda3/envs/tensorflow/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x16a0f0ab8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "# 0. Import Dependencies\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "from moviepy.editor import *\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "from moviepy.video.fx.all import speedx\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2b1b9f",
   "metadata": {},
   "source": [
    "# 1 - Configuración global, rutas y semillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c04fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTEBOOK_DIR      : /Users/armandobecerril/PhD/MSL-150/notebooks\n",
      "BASE_DIR          : /Users/armandobecerril/PhD/MSL-150\n",
      "DATA_DIR          : /Users/armandobecerril/PhD/MSL-150/data\n",
      "RAW_VIDEO_DIR     : /Users/armandobecerril/PhD/MSL-150/data/raw\n",
      "RAW_NPY_DIR       : /Users/armandobecerril/PhD/MSL-150/data/raw_npy\n",
      "SAMPLE_NPY_DIR    : /Users/armandobecerril/PhD/MSL-150/data/sample_npy\n",
      "ORIGINAL_SOURCE_DIR: /Users/armandobecerril/PhD/MSL-150/data/original_source\n",
      "CASES_DIR         : /Users/armandobecerril/PhD/MSL-150/data/cases\n",
      "Random seeds set to: 42\n"
     ]
    }
   ],
   "source": [
    "# 1. Initial Configuration and Experiment Scope\n",
    "# ------------------------------------------------------------\n",
    "# This cell defines:\n",
    "# - BASE_DIR: repository root (one level above `notebooks/`)\n",
    "# - DATA_DIR: main data directory\n",
    "# - RAW_VIDEO_DIR: full raw videos (not included in the public repo)\n",
    "# - RAW_NPY_DIR: full keypoint tensors\n",
    "# - SAMPLE_NPY_DIR: small public subset for reproducibility\n",
    "# - RANDOM SEEDs for NumPy, TensorFlow and Python\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "# Resolve base directory *relative* to this notebook\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent          # MSL-150 root\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "\n",
    "RAW_VIDEO_DIR = DATA_DIR / \"raw\"        # full set of original videos (local only)\n",
    "RAW_NPY_DIR = DATA_DIR / \"raw_npy\"      # full npy export\n",
    "SAMPLE_NPY_DIR = DATA_DIR / \"sample_npy\"  # small subset for GitHub\n",
    "\n",
    "DICTIONARY_DIR = DATA_DIR / \"dictionary\"\n",
    "ORIGINAL_SOURCE_DIR = DATA_DIR / \"original_source\"   # anonymized vocabulary samples\n",
    "CASES_DIR = DATA_DIR / \"cases\"                       # anonymized narrative cases\n",
    "\n",
    "# Output synthetic samples data\n",
    "SYNTH_DATA = os.path.join(BASE_DIR, \"data\", \"synthetic_sample_data\")\n",
    "\n",
    "TERMS_PATH = os.path.join(BASE_DIR, \"data\", \"dictionary\", \"terms.txt\")\n",
    "TERMS_SAMPLE_PATH = os.path.join(BASE_DIR, \"data\", \"dictionary\", \"terms_sample.txt\")\n",
    "\n",
    "USE_SAMPLE = True  # Cambia a False para usar vocabulario completo\n",
    "TERMS_PATH = TERMS_SAMPLE_PATH if USE_SAMPLE else TERMS_PATH\n",
    "\n",
    "\n",
    "print(\"NOTEBOOK_DIR      :\", NOTEBOOK_DIR)\n",
    "print(\"BASE_DIR          :\", BASE_DIR)\n",
    "print(\"DATA_DIR          :\", DATA_DIR)\n",
    "print(\"RAW_VIDEO_DIR     :\", RAW_VIDEO_DIR)\n",
    "print(\"RAW_NPY_DIR       :\", RAW_NPY_DIR)\n",
    "print(\"SAMPLE_NPY_DIR    :\", SAMPLE_NPY_DIR)\n",
    "print(\"ORIGINAL_SOURCE_DIR:\", ORIGINAL_SOURCE_DIR)\n",
    "print(\"CASES_DIR         :\", CASES_DIR)\n",
    "\n",
    "# Create output directories if they do not exist (safe for public repo)\n",
    "RAW_NPY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SAMPLE_NPY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Global config: reproducibility\n",
    "RANDOM_SEED = 42\n",
    "frames=30\n",
    "samples=20\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Random seeds set to:\", RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db6f479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de términos en videos_dict_aug: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ambulancia', 20), ('doctor', 20), ('dolor', 20), ('hoy', 20), ('yo', 20)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leer términos desde TERMS_PATH y construir el diccionario de vocabulario\n",
    "videos_dict_aug = {}\n",
    "\n",
    "with open(TERMS_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        term = line.strip()\n",
    "        if term:\n",
    "            videos_dict_aug[term] = samples  # p.ej. 800 , 200 o 20 según tu escenario\n",
    "\n",
    "print(\"Número de términos en videos_dict_aug:\", len(videos_dict_aug))\n",
    "list(videos_dict_aug.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccdae20",
   "metadata": {},
   "source": [
    "# 2. Synthetic sample video generation for MSL-150\n",
    "-------------------------------------------------------------\n",
    "\n",
    "This script takes one blurred source video per sign (e.g. \"ambulancia_blur.mp4\")\n",
    "from:\n",
    "\n",
    "    data/original_source/\n",
    "\n",
    "and generates up to N augmented videos per sign under:\n",
    "\n",
    "    data/synthetic_sample_videos/<term>/<term>_XXX.mp4\n",
    "\n",
    "Each synthetic clip applies:\n",
    "- small random speed variations (0.85x–1.15x),\n",
    "- a small random rotation (-10° to +10°),\n",
    "- frame-wise Gaussian noise.\n",
    "\n",
    "This sample-friendly pipeline reproduces the main augmentation logic used in the\n",
    "full MSL-150 dataset, but only for the five public signs:\n",
    "\n",
    "    ambulancia, doctor, dolor, hoy, yo\n",
    "\n",
    "so reviewers and other researchers can inspect an end-to-end version of the\n",
    "process without accessing the full private dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d90aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TERM] ambulancia: existing=20, need=0\n",
      "[SKIP] Already have required number of videos.\n",
      "\n",
      "[TERM] doctor: existing=20, need=0\n",
      "[SKIP] Already have required number of videos.\n",
      "\n",
      "[TERM] dolor: existing=20, need=0\n",
      "[SKIP] Already have required number of videos.\n",
      "\n",
      "[TERM] hoy: existing=20, need=0\n",
      "[SKIP] Already have required number of videos.\n",
      "\n",
      "[TERM] yo: existing=20, need=0\n",
      "[SKIP] Already have required number of videos.\n",
      "\n",
      "[DONE] All synthetic sample videos generated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from moviepy.editor import VideoFileClip\n",
    "import moviepy.video.fx.all as vfx\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1. Paths and target configuration\n",
    "# --------------------------------------------------------------------\n",
    "BASE_DIR = \"/Users/armandobecerril/PhD/MSL-150\"\n",
    "\n",
    "# Base source videos\n",
    "ORIGINAL_DIR = os.path.join(BASE_DIR, \"data\", \"original_source\")\n",
    "\n",
    "# Output synthetic samples\n",
    "SYNTH_DIR = os.path.join(BASE_DIR, \"data\", \"synthetic_sample_videos\")\n",
    "\n",
    "# Number of synthetic videos per sign\n",
    "videos_dict_aug = {\n",
    "    \"ambulancia\": samples,\n",
    "    \"doctor\": samples,\n",
    "    \"dolor\": samples,\n",
    "    \"hoy\": samples,\n",
    "    \"yo\": samples,\n",
    "}\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2. Utility functions\n",
    "# --------------------------------------------------------------------\n",
    "def ensure_directory_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"[INFO] Created directory: {directory}\")\n",
    "\n",
    "\n",
    "def add_gaussian_noise(image, mean=0, sigma=25):\n",
    "    \"\"\"Add Gaussian noise to a frame.\"\"\"\n",
    "    gauss = np.random.normal(mean, sigma, image.shape)\n",
    "    noisy = np.clip(image.astype(np.float32) + gauss, 0, 255).astype(np.uint8)\n",
    "    return noisy\n",
    "\n",
    "\n",
    "def transform_clip(clip):\n",
    "    \"\"\"Apply augmentations: speed, rotation, and Gaussian noise.\"\"\"\n",
    "\n",
    "    # 1) Random speed\n",
    "    speed_factor = np.random.uniform(0.85, 1.15)\n",
    "    clip = clip.fx(vfx.speedx, speed_factor)\n",
    "\n",
    "    # 2) Random rotation\n",
    "    rotation_deg = np.random.uniform(-10, 10)\n",
    "    clip = clip.fx(vfx.rotate, rotation_deg)\n",
    "\n",
    "    # 3) Gaussian noise per frame\n",
    "    def add_noise(get_frame, t):\n",
    "        return add_gaussian_noise(get_frame(t))\n",
    "\n",
    "    clip = clip.fl(add_noise)\n",
    "    return clip\n",
    "\n",
    "\n",
    "def is_valid_filename(filename, term):\n",
    "    \"\"\"\n",
    "    Accept filenames of the form <term>_XXX.mp4\n",
    "    \"\"\"\n",
    "    if not filename.lower().endswith(\".mp4\"):\n",
    "        return False\n",
    "\n",
    "    name, _ = os.path.splitext(filename)\n",
    "    parts = name.split(\"_\")\n",
    "\n",
    "    if len(parts) != 2:\n",
    "        return False\n",
    "\n",
    "    prefix, idx = parts\n",
    "    return prefix == term and idx.isdigit()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3. Main generation routine\n",
    "# --------------------------------------------------------------------\n",
    "def manage_videos(original_dir, synth_root, videos_dict_aug):\n",
    "\n",
    "    ensure_directory_exists(synth_root)\n",
    "\n",
    "    for term, num_samples in videos_dict_aug.items():\n",
    "\n",
    "        term_dir = os.path.join(synth_root, term)\n",
    "        ensure_directory_exists(term_dir)\n",
    "\n",
    "        # List current synthetic samples\n",
    "        existing = sorted(\n",
    "            f for f in os.listdir(term_dir)\n",
    "            if is_valid_filename(f, term)\n",
    "        )\n",
    "\n",
    "        needed = num_samples - len(existing)\n",
    "        print(f\"\\n[TERM] {term}: existing={len(existing)}, need={needed}\")\n",
    "\n",
    "        if needed <= 0:\n",
    "            print(\"[SKIP] Already have required number of videos.\")\n",
    "            continue\n",
    "\n",
    "        # Base video — IMPORTANT change — now <term>_001.mp4\n",
    "        base_video_path = os.path.join(original_dir, f\"{term}_001.mp4\")\n",
    "\n",
    "        if not os.path.exists(base_video_path):\n",
    "            print(f\"[ERROR] Base video not found for: {base_video_path}\")\n",
    "            continue\n",
    "\n",
    "        start_idx = len(existing) + 1  # continue numbering\n",
    "\n",
    "        for i in range(start_idx, num_samples + 1):\n",
    "            out_path = os.path.join(term_dir, f\"{term}_{i:03}.mp4\")\n",
    "\n",
    "            print(f\"[GEN] Creating: {out_path}\")\n",
    "            with VideoFileClip(base_video_path) as clip:\n",
    "                aug_clip = transform_clip(clip)\n",
    "                aug_clip.write_videofile(\n",
    "                    out_path,\n",
    "                    codec=\"libx264\",\n",
    "                    audio=False,\n",
    "                    verbose=False,\n",
    "                    logger=None\n",
    "                )\n",
    "\n",
    "    print(\"\\n[DONE] All synthetic sample videos generated.\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4. Execute\n",
    "# --------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    manage_videos(ORIGINAL_DIR, SYNTH_DIR, videos_dict_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60950abb",
   "metadata": {},
   "source": [
    "# 3. MediaPipe Keypoints & Project Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d5629",
   "metadata": {},
   "source": [
    "For all videos, we used MediaPipe Holistic to extract 3D keypoints from the upper body and both hands. Each frame was preprocessed using a dedicated wrapper (mediapipe_detection) that converts the image from BGR to RGB, runs holistic inference, and returns both the annotated frame and the landmark structures. We then converted the pose and hand landmarks into a fixed-length feature vector with extract_keypoints_lsm, concatenating 25 pose landmarks (x, y, z, visibility) and 21×2 hand landmarks (x, y, z) and zero-padding missing detections. To reduce temporal noise, we applied a central cropping procedure (central_clean_data) that removes low-information frames and selects a centered window of 30 frames per clip. The resulting frame-level features were first stored in CSV format and later converted into NumPy sequences with extract_keypoints_lsm_from_csv and save_keypoints_to_npy, which organize the data in a hierarchical directory of .npy files grouped by sign and video sample. This preprocessing pipeline, fully released in our GitHub repository, standardizes all inputs for training and evaluating the LSTM/GRU models reported in this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ac3b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities4\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    lineDrawingSpec = mp_drawing.DrawingSpec(thickness=1, color=(255,255,0))\n",
    "    pointDrawingSpec1 = mp_drawing.DrawingSpec(color=(255,255,0), thickness=1, circle_radius=1)\n",
    "    pointDrawingSpec2 = mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=1)\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "    \n",
    "def draw_landmarks_lsm(image, results):\n",
    "    lineDrawingSpec = mp_drawing.DrawingSpec(thickness=1, color=(255,255,0))\n",
    "    pointDrawingSpec1 = mp_drawing.DrawingSpec(color=(255,255,0), thickness=1, circle_radius=1)\n",
    "    pointDrawingSpec2 = mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=1)\n",
    "    #mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "    \n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(255,255,0), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(255,255,0), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks,mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=2), \n",
    "                             mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=2), \n",
    "                             mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "\n",
    "def draw_styled_landmarks_lsm(image, results):\n",
    "    \n",
    "    # Draw face connections  \n",
    "    #mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS, \n",
    "    #mp_drawing.DrawingSpec(color=(255,255,0), thickness=1, circle_radius=1), \n",
    "    #mp_drawing.DrawingSpec(color=(255,255,0), thickness=1)\n",
    "    #                         ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks,mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=2), \n",
    "                             mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=2), \n",
    "                             mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "\n",
    "def get_column_headers():\n",
    "    # Set column headers for CSV file\n",
    "    column_headers = ['VIDEO_SAMPLE ','CLASSIFICATION', 'FRAME', 'TIMESTAMP']\n",
    "    \n",
    "    for point_rh in mp_holistic.HandLandmark:\n",
    "        column_headers += [f'RIGHT_{point_rh.name}_X', f'RIGHT_{point_rh.name}_Y', f'RIGHT_{point_rh.name}_Z']\n",
    "    \n",
    "    for point_lh in mp_holistic.HandLandmark:\n",
    "        column_headers += [f'LEFT_{point_lh.name}_X', f'LEFT_{point_lh.name}_Y', f'LEFT_{point_lh.name}_Z']\n",
    "    \n",
    "    for point in mp_holistic.PoseLandmark:\n",
    "        if point.value < 25 or point.value > 32:\n",
    "            column_headers += [f'{point.name}_X', f'{point.name}_Y', f'{point.name}_Z', f'{point.name}_V']\n",
    "            \n",
    "    return column_headers\n",
    "\n",
    "def get_max_video_sample(file_path):\n",
    "    \"\"\" Obtiene el máximo número de 'video_sample' registrado en el archivo CSV. \"\"\"\n",
    "    max_sample = 0\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # Skip header\n",
    "            for row in reader:\n",
    "                if row:\n",
    "                    current_sample = int(row[0])\n",
    "                    if current_sample > max_sample:\n",
    "                        max_sample = current_sample\n",
    "    return max_sample\n",
    "\n",
    "def extract_keypoints_lsm(results):\n",
    "    # mano derecha\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "    # mano izquierda\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "    # Pose (excluding landmarks 25-32)\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for idx, res in enumerate(results.pose_landmarks.landmark) if idx < 25 or idx > 32]).flatten() if results.pose_landmarks else np.zeros(25*4)\n",
    "\n",
    "    return np.concatenate([pose, lh, rh])\n",
    "\n",
    "def ensure_directory_exists(directory):\n",
    "    \"\"\"\n",
    "    Ensure the directory exists.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): The full path to the directory to ensure exists.\n",
    "        \n",
    "    Returns:\n",
    "        str: The path to the directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    return directory\n",
    "\n",
    "#Data Standarization 30 Frames\n",
    "#Se estandrariza e integran archivos a partir de un número de Frames dado, reduciendo de los extremos del video que es donde predominan más ceros\n",
    "\n",
    "def central_clean_data(data, frames):\n",
    "    non_zero_rows = data[(data.drop(\"VIDEO_SAMPLE\", axis=1) != 0).sum(axis=1) > data.shape[1] * 0.5]\n",
    "    \n",
    "    num_rows = len(non_zero_rows)\n",
    "    if num_rows < frames:\n",
    "        # Si hay menos filas que 'frames', simplemente devuelve todos los datos.\n",
    "        return non_zero_rows\n",
    "    else:\n",
    "        # Calcula el inicio y el final para centrar los datos en 'frames'.\n",
    "        start_idx = (num_rows - frames) // 2\n",
    "        end_idx = start_idx + frames\n",
    "        return non_zero_rows.iloc[start_idx:end_idx]\n",
    "    \n",
    "# Data Preparation Directories & Numpy\n",
    "def extract_keypoints_lsm_from_csv(df, classification, video_sample_num):\n",
    "    # Filtra el dataframe por la clasificación solicitada\n",
    "    filtered_data = df[(df['CLASSIFICATION'] == classification) & (df['VIDEO_SAMPLE'] == video_sample_num)]\n",
    "\n",
    "    # Inicializa matrices vacías si no hay datos\n",
    "    if filtered_data.empty:\n",
    "        pose_data = np.zeros((1, 100))  # Asumiendo 25 keypoints x 4 datos cada uno\n",
    "        lh_data = np.zeros((1, 63))  # Asumiendo 21 keypoints x 3 datos cada uno\n",
    "        rh_data = np.zeros((1, 63))  # Asumiendo 21 keypoints x 3 datos cada uno\n",
    "    else:\n",
    "        # Extraer datos de mano derecha, mano izquierda y pose usando índices fijos\n",
    "        rh_data = filtered_data.iloc[:, 4:67].values   # Columns 4-66 (0-indexed)\n",
    "        lh_data = filtered_data.iloc[:, 67:130].values # Columns 67-129\n",
    "        pose_data = filtered_data.iloc[:, 130:230].values # Columns 130-229\n",
    "\n",
    "    # Concatenar y devolver\n",
    "    return np.concatenate([pose_data, lh_data, rh_data], axis=1)\n",
    "\n",
    "\n",
    "def save_keypoints_to_npy(df, output_base_path):\n",
    "    classifications = df['CLASSIFICATION'].unique()\n",
    "    \n",
    "    for classification in classifications:\n",
    "        class_dir = os.path.join(output_base_path, str(classification))\n",
    "        if not os.path.exists(class_dir):\n",
    "            os.makedirs(class_dir)\n",
    "        \n",
    "        video_samples_for_classification = df[df['CLASSIFICATION'] == classification]['VIDEO_SAMPLE'].unique()\n",
    "        \n",
    "        for video_sample in video_samples_for_classification:\n",
    "            keypoints_data = extract_keypoints_lsm_from_csv(df, classification, video_sample)\n",
    "            #print(f\"Total features for {classification}-{video_sample}:\", keypoints_data.shape[1])\n",
    "            \n",
    "            sample_dir = os.path.join(class_dir, str(video_sample))\n",
    "            if not os.path.exists(sample_dir):\n",
    "                os.makedirs(sample_dir)\n",
    "            \n",
    "            for idx, keypoints in enumerate(keypoints_data):\n",
    "                np.save(os.path.join(sample_dir, f\"{idx}.npy\"), keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68437a04",
   "metadata": {},
   "source": [
    "# 4. Data Extraction (Build CSV Data Base for Mexican Sign Language)\n",
    "Toma los videos sintéticos de muestra generados en data/synthetic_sample_videos/<term>/<term>_XXX.mp4.\n",
    "\n",
    "Para cada término y cada video:\n",
    "\n",
    "Recorre frame por frame.\n",
    "\n",
    "Ejecuta MediaPipe Holistic y extrae:\n",
    "\n",
    "3D landmarks de mano derecha (21×3),\n",
    "\n",
    "mano izquierda (21×3),\n",
    "\n",
    "y un subconjunto de landmarks de pose (25×4).\n",
    "\n",
    "Construye una fila por frame con:\n",
    "\n",
    "VIDEO_SAMPLE, CLASSIFICATION, FRAME, TIMESTAMP, y todos los features.\n",
    "\n",
    "Guarda todo en CSV por término en\n",
    "data/synthetic_sample_data/<term>/<term>_lsm.csv, con un header compatible con el resto del pipeline que después transforma estos CSV en secuencias .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c77f59ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] CSV header created at: /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "[INFO] Processing class='ambulancia' from VIDEO_SAMPLE=1 to 20 (synthetic_sample_videos).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Appended 69 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 63 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 63 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 62 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 60 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 78 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 66 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 66 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 65 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 61 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 61 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 64 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 60 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 65 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 72 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 66 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 64 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 75 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 75 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "  [OK] Appended 66 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "[INFO] CSV header created at: /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "[INFO] Processing class='doctor' from VIDEO_SAMPLE=1 to 20 (synthetic_sample_videos).\n",
      "  [OK] Appended 62 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 57 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 71 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 66 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 55 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 67 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 71 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 62 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 70 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 58 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 57 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 64 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 72 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 67 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 68 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 55 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 63 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 71 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 57 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "  [OK] Appended 57 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "[INFO] CSV header created at: /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "[INFO] Processing class='dolor' from VIDEO_SAMPLE=1 to 20 (synthetic_sample_videos).\n",
      "  [OK] Appended 59 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 69 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 59 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 62 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 67 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 54 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 66 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 59 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 67 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 54 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 58 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 57 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 54 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 55 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 62 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 58 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 63 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 58 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 72 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "  [OK] Appended 61 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "[INFO] CSV header created at: /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "[INFO] Processing class='hoy' from VIDEO_SAMPLE=1 to 20 (synthetic_sample_videos).\n",
      "  [OK] Appended 56 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 74 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 73 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 75 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 69 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 67 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 64 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Appended 64 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 72 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 57 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 62 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 63 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 58 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 59 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 58 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 60 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 63 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 66 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 72 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "  [OK] Appended 61 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "[INFO] CSV header created at: /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "[INFO] Processing class='yo' from VIDEO_SAMPLE=1 to 20 (synthetic_sample_videos).\n",
      "  [OK] Appended 65 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 53 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 64 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 50 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 52 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 60 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 60 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 61 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 64 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 64 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 59 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 61 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 55 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 53 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 59 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 55 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 62 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 51 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 61 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "  [OK] Appended 57 frames to /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Supuestos: ya tienes definidas en el notebook:\n",
    "#   - mp_holistic\n",
    "#   - mediapipe_detection(image, model)\n",
    "#   - draw_styled_landmarks_lsm(image, results)\n",
    "#   - ensure_directory_exists(path)\n",
    "#   - get_column_headers()\n",
    "#   - videos_dict_aug = {\"ambulancia\": 20, \"doctor\": 20, ...}  # para samples\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "BASE_DIR = \"/Users/armandobecerril/PhD/MSL-150\"\n",
    "\n",
    "SYNTH_VIDEOS = os.path.join(BASE_DIR, \"data\", \"synthetic_sample_videos\")\n",
    "SYNTH_DATA   = os.path.join(BASE_DIR, \"data\", \"synthetic_sample_data\")\n",
    "\n",
    "# Directorio raíz donde se guardarán los CSV por término\n",
    "std_dir = ensure_directory_exists(SYNTH_DATA)\n",
    "\n",
    "# Mostrar ventana de QA visual (pon en False para correr rápido sin GUI)\n",
    "SHOW_VIDEO = True\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) Utility: obtener máximo VIDEO_SAMPLE ya presente en un CSV\n",
    "# ---------------------------------------------------------\n",
    "def get_max_video_sample(file_path):\n",
    "    \"\"\"\n",
    "    Obtiene el máximo valor de VIDEO_SAMPLE ya registrado en el CSV.\n",
    "    Si el archivo no existe o está vacío, regresa 0.\n",
    "\n",
    "    Se asume que la primera columna es VIDEO_SAMPLE.\n",
    "    \"\"\"\n",
    "    max_sample = 0\n",
    "    if not os.path.exists(file_path):\n",
    "        return 0\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        try:\n",
    "            next(reader)  # saltar header\n",
    "        except StopIteration:\n",
    "            return 0\n",
    "\n",
    "        for row in reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            try:\n",
    "                current_sample = int(row[0])\n",
    "                if current_sample > max_sample:\n",
    "                    max_sample = current_sample\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    return max_sample\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) Asegurar header correcto usando get_column_headers()\n",
    "# ---------------------------------------------------------\n",
    "def ensure_csv_header(output_csv_path):\n",
    "    \"\"\"\n",
    "    Crea el CSV con el header semántico completo si el archivo\n",
    "    no existe o está vacío.\n",
    "\n",
    "    Usa la función get_column_headers() ya existente en el proyecto.\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_csv_path) and os.path.getsize(output_csv_path) > 0:\n",
    "        return  # ya tiene contenido\n",
    "\n",
    "    # Usamos tu función y de paso hacemos strip() por si hay espacios en 'VIDEO_SAMPLE '\n",
    "    header = [h.strip() for h in get_column_headers()]\n",
    "\n",
    "    with open(output_csv_path, mode=\"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(header)\n",
    "\n",
    "    print(f\"[INFO] CSV header created at: {output_csv_path}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Loop principal: synthetic_sample_videos → synthetic_sample_data\n",
    "# ---------------------------------------------------------\n",
    "for classification, num_videos in videos_dict_aug.items():\n",
    "    # Subdirectorio para los CSV de esta clasificación (ambulancia, doctor, etc.)\n",
    "    class_dir = ensure_directory_exists(os.path.join(std_dir, classification))\n",
    "    output_csv_path = os.path.join(class_dir, f\"{classification}_lsm.csv\")\n",
    "\n",
    "    # Crear header si el CSV es nuevo / vacío\n",
    "    ensure_csv_header(output_csv_path)\n",
    "\n",
    "    # Averiguar el máximo VIDEO_SAMPLE ya procesado\n",
    "    existing_max_sample = get_max_video_sample(output_csv_path)\n",
    "    target_samples = num_videos  # por ejemplo, 20 para los samples\n",
    "\n",
    "    if existing_max_sample >= target_samples:\n",
    "        print(\n",
    "            f\"[SKIP] CSV {output_csv_path} already has {existing_max_sample} samples \"\n",
    "            f\"(target = {target_samples}). Skipping '{classification}'...\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    start_video_num = existing_max_sample + 1\n",
    "\n",
    "    print(\n",
    "        f\"[INFO] Processing class='{classification}' from VIDEO_SAMPLE={start_video_num} \"\n",
    "        f\"to {target_samples} (synthetic_sample_videos).\"\n",
    "    )\n",
    "\n",
    "    for video_num in range(start_video_num, target_samples + 1):\n",
    "        # Ruta al video sintético: data/synthetic_sample_videos/<term>/<term>_XXX.mp4\n",
    "        video_path = os.path.join(\n",
    "            SYNTH_VIDEOS,\n",
    "            classification,\n",
    "            f\"{classification}_{video_num:03}.mp4\"\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"  [WARN] Video not found: {video_path}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # -----------------------------\n",
    "        # Open video capture\n",
    "        # -----------------------------\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"  [WARN] Could not open: {video_path}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        frame_no = 0\n",
    "        data_exp = []\n",
    "\n",
    "        # MediaPipe Holistic\n",
    "        with mp_holistic.Holistic(\n",
    "            min_detection_confidence=0.85,\n",
    "            min_tracking_confidence=0.25,\n",
    "            model_complexity=0\n",
    "        ) as holistic:\n",
    "\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break  # fin del video\n",
    "\n",
    "                # Detección Mediapipe\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # QA visual (opcional)\n",
    "                draw_styled_landmarks_lsm(image, results)\n",
    "\n",
    "                if SHOW_VIDEO:\n",
    "                    win_name = f\"{classification}_{video_num:03d}\"\n",
    "                    cv2.imshow(win_name, image)\n",
    "                    # Con 'q' sales de ese video\n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "\n",
    "                # Timestamp en ms\n",
    "                timestamp = round(cap.get(cv2.CAP_PROP_POS_MSEC), 4)\n",
    "\n",
    "                # Campos básicos\n",
    "                video_sample = [video_num]\n",
    "                classificacion = [classification]\n",
    "                tiempo = [frame_no, timestamp]\n",
    "\n",
    "                # Mano derecha (21 keypoints × 3 coords)\n",
    "                rh = list(\n",
    "                    np.array(\n",
    "                        [[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]\n",
    "                    ).flatten()\n",
    "                ) if results.right_hand_landmarks else list(np.zeros(21 * 3))\n",
    "\n",
    "                # Mano izquierda\n",
    "                lh = list(\n",
    "                    np.array(\n",
    "                        [[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]\n",
    "                    ).flatten()\n",
    "                ) if results.left_hand_landmarks else list(np.zeros(21 * 3))\n",
    "\n",
    "                # Pose (sin 25–32, igual que en tu pipeline original)\n",
    "                pose = list(\n",
    "                    np.array(\n",
    "                        [\n",
    "                            [res.x, res.y, res.z, res.visibility]\n",
    "                            for idx, res in enumerate(results.pose_landmarks.landmark)\n",
    "                            if idx < 25 or idx > 32\n",
    "                        ]\n",
    "                    ).flatten()\n",
    "                ) if results.pose_landmarks else list(np.zeros(25 * 4))\n",
    "\n",
    "                row = video_sample + classificacion + tiempo + rh + lh + pose\n",
    "                data_exp.append(row)\n",
    "                frame_no += 1\n",
    "\n",
    "        cap.release()\n",
    "        if SHOW_VIDEO:\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Append de todas las filas de ese video al CSV\n",
    "        # -----------------------------\n",
    "        try:\n",
    "            with open(output_csv_path, mode=\"a\", newline=\"\") as f:\n",
    "                csv_writer = csv.writer(\n",
    "                    f,\n",
    "                    delimiter=\",\",\n",
    "                    quotechar='\"',\n",
    "                    quoting=csv.QUOTE_MINIMAL\n",
    "                )\n",
    "                csv_writer.writerows(data_exp)\n",
    "            print(f\"  [OK] Appended {len(data_exp)} frames to {output_csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] Writing to {output_csv_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6981c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Safe close\n",
    "try:\n",
    "    cap.release()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# A tiny delay — required on macOS to force window close\n",
    "for i in range(5):\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d415af22",
   "metadata": {},
   "source": [
    "# 4.1. Quality control and consistency checks for processed CSV files\n",
    "\n",
    "Before training, we applied a two-step quality control procedure over the per-sign CSV files generated from the synthetic videos.\n",
    "\n",
    "First, we normalized the schema of all CSV files by converting column headers to uppercase and trimming whitespace. This guarantees a consistent set of variable names (e.g., VIDEO_SAMPLE, CLASSIFICATION, FRAME, TIMESTAMP, and the pose/hand keypoint columns) across all signs and synthetic samples.\n",
    "\n",
    "Second, we verified coverage and integrity for each sign category using the target number of synthetic videos defined in videos_dict_aug. For every sign, the script checks that the expected CSV file exists and that all VIDEO_SAMPLE identifiers from 1 to N (e.g., 1…200) are present. Missing CSVs or missing sample IDs are reported explicitly. Only after all CSVs pass this normalization and coverage check do we proceed to aggregate data and export the final NumPy tensors used for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b4e5377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Headers normalized for: /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv\n",
      "[OK] Headers normalized for: /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv\n",
      "[OK] Headers normalized for: /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv\n",
      "[OK] Headers normalized for: /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv\n",
      "[OK] Headers normalized for: /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv\n",
      "[OK] ambulancia: CSV /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/ambulancia/ambulancia_lsm.csv has samples 1..20.\n",
      "[OK] doctor: CSV /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/doctor/doctor_lsm.csv has samples 1..20.\n",
      "[OK] dolor: CSV /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/dolor/dolor_lsm.csv has samples 1..20.\n",
      "[OK] hoy: CSV /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/hoy/hoy_lsm.csv has samples 1..20.\n",
      "[OK] yo: CSV /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data/yo/yo_lsm.csv has samples 1..20.\n",
      "\n",
      "All CSV files are present and consistent.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Paths (MSL-150 project)\n",
    "# -------------------------------------------------------------------\n",
    "BASE_DIR   = \"/Users/armandobecerril/PhD/MSL-150\"\n",
    "SYNTH_DATA = os.path.join(BASE_DIR, \"data\", \"synthetic_sample_data\")\n",
    "\n",
    "# videos_dict_aug must already be defined in the notebook:\n",
    "# videos_dict_aug = {\"AMBULANCIA\": SAMPLES , \"DOCTOR\": SAMPLES, ...}\n",
    "\n",
    "\n",
    "def update_csv_headers_to_upper_and_trim(root_dir: str):\n",
    "    \"\"\"\n",
    "    Normalize the schema of all CSV files under `root_dir` by:\n",
    "      - Converting column names to UPPERCASE.\n",
    "      - Stripping leading/trailing whitespace from column names.\n",
    "    \n",
    "    This step ensures that downstream code can rely on a\n",
    "    consistent schema (e.g. VIDEO_SAMPLE, CLASSIFICATION, FRAME, TIMESTAMP, ...).\n",
    "    \"\"\"\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if not file.endswith(\".csv\"):\n",
    "                continue\n",
    "\n",
    "            filepath = os.path.join(subdir, file)\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "\n",
    "                # Normalize headers\n",
    "                df.columns = [col.upper().strip() for col in df.columns]\n",
    "\n",
    "                df.to_csv(filepath, index=False)\n",
    "                print(f\"[OK] Headers normalized for: {filepath}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERR] Could not update headers for {filepath}: {e}\")\n",
    "\n",
    "\n",
    "def check_csv_coverage(root_dir: str, videos_dict_aug: dict):\n",
    "    \"\"\"\n",
    "    For each classification in `videos_dict_aug`, verify:\n",
    "      1) The CSV file <classification>/<classification>_lsm.csv exists.\n",
    "      2) All VIDEO_SAMPLE IDs from 1 to expected_samples are present.\n",
    "\n",
    "    Prints detailed messages for missing files or missing samples.\n",
    "    \"\"\"\n",
    "    missing_classes = []\n",
    "\n",
    "    for classification, expected_samples in videos_dict_aug.items():\n",
    "        csv_path = os.path.join(root_dir, classification, f\"{classification}_lsm.csv\")\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"[MISSING CSV] {classification}: {csv_path} not found.\")\n",
    "            missing_classes.append(classification)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            # Make sure column name is normalized\n",
    "            df.columns = [c.upper().strip() for c in df.columns]\n",
    "\n",
    "            if \"VIDEO_SAMPLE\" not in df.columns:\n",
    "                print(f\"[ERR] 'VIDEO_SAMPLE' column not found in {csv_path}\")\n",
    "                missing_classes.append(classification)\n",
    "                continue\n",
    "\n",
    "            samples_present = sorted(df[\"VIDEO_SAMPLE\"].unique().tolist())\n",
    "            missing_samples = [\n",
    "                sample_id\n",
    "                for sample_id in range(1, expected_samples + 1)\n",
    "                if sample_id not in samples_present\n",
    "            ]\n",
    "\n",
    "            if missing_samples:\n",
    "                print(\n",
    "                    f\"[WARN] {classification}: missing VIDEO_SAMPLE IDs: \"\n",
    "                    f\"{missing_samples}\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"[OK] {classification}: CSV {csv_path} has samples 1..{expected_samples}.\"\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERR] Could not read {csv_path}: {e}\")\n",
    "            missing_classes.append(classification)\n",
    "\n",
    "    # Summary\n",
    "    if missing_classes:\n",
    "        print(\"\\nSummary – Missing or inconsistent classifications:\")\n",
    "        print(\", \".join(sorted(missing_classes)))\n",
    "    else:\n",
    "        print(\"\\nAll CSV files are present and consistent.\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Execute the 4.1 quality & coverage checks on the synthetic samples\n",
    "# -------------------------------------------------------------------\n",
    "update_csv_headers_to_upper_and_trim(SYNTH_DATA)\n",
    "check_csv_coverage(SYNTH_DATA, videos_dict_aug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a7b8a",
   "metadata": {},
   "source": [
    "# 5. Data Consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac574f",
   "metadata": {},
   "source": [
    "Temporal standardization and consolidated CSV generation\n",
    "\n",
    "For each sign category, the synthetic videos were first converted into frame-level CSV files containing the full sequence of pose and hand keypoints produced by MediaPipe Holistic. To ensure temporal consistency across all samples, we applied a central trimming strategy over the time dimension.\n",
    "\n",
    "Specifically, for every (CLASSIFICATION, VIDEO_SAMPLE) pair we removed leading and trailing frames dominated by zeros (i.e., frames where more than 50% of the keypoint features were zero), and then extracted a fixed window of 30 central frames. Samples with fewer than 30 valid frames after this cleaning step were flagged as incomplete and reported, but the vast majority of sequences met the minimum length requirement.\n",
    "\n",
    "The standardized sequences (30 frames × all keypoints) were then concatenated across all sign categories to build a single consolidated CSV file, MSL-150_Mexican_Sign_Language_Dataset.csv, which includes the fields VIDEO_SAMPLE, CLASSIFICATION, FRAME, TIMESTAMP and the full set of pose and hand coordinates. Additional validation scripts confirmed that each sign reached the expected number of synthetic samples and that each sample contained at least 30 frames, before exporting the NumPy tensors used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "013734ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Consolidated CSV written to:\n",
      "  /Users/armandobecerril/PhD/MSL-150/data/synthetic_sample_data_total/MSL-150_Mexican_Sign_Language_Dataset.csv\n",
      "\n",
      "Summary of sequences with fewer than 30 frames (after cleaning):\n",
      "  ambulancia: 11 incomplete samples\n",
      "  doctor: 9 incomplete samples\n",
      "  dolor: 2 incomplete samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Paths for the “sample” dataset inside the MSL-150 repo\n",
    "# ------------------------------------------------------------\n",
    "BASE_DIR        = \"/Users/armandobecerril/PhD/MSL-150\"\n",
    "SYNTH_DATA_DIR  = os.path.join(BASE_DIR, \"data\", \"synthetic_sample_data\")\n",
    "TOTAL_DATA_DIR  = os.path.join(BASE_DIR, \"data\", \"synthetic_sample_data_total\")\n",
    "os.makedirs(TOTAL_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Final consolidated CSV (small sample version for the repo)\n",
    "FINAL_CSV_PATH = os.path.join(\n",
    "    TOTAL_DATA_DIR,\n",
    "    \"MSL-150_Mexican_Sign_Language_Dataset.csv\"\n",
    ")\n",
    "\n",
    "def central_clean_data(df, frames=30):\n",
    "    \"\"\"\n",
    "    Temporal standardization:\n",
    "    - Remove frames that are mostly zeros (over all keypoint columns).\n",
    "    - Extract a centered window of exactly `frames` rows whenever possible.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cleaned_df : pd.DataFrame\n",
    "        Either the centered window or the non-zero subset if shorter than `frames`.\n",
    "    is_complete : bool\n",
    "        True if at least `frames` valid rows were available.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df, False\n",
    "\n",
    "    # Consider all columns except VIDEO_SAMPLE to detect \"non-zero\" frames\n",
    "    non_zero_rows = df[\n",
    "        (df.drop(\"VIDEO_SAMPLE\", axis=1) != 0).sum(axis=1) > df.shape[1] * 0.5\n",
    "    ]\n",
    "\n",
    "    num_rows = len(non_zero_rows)\n",
    "    if num_rows < frames:\n",
    "        # Not enough valid frames; return what we have and mark as incomplete\n",
    "        return non_zero_rows, False\n",
    "    else:\n",
    "        # Centered window of exactly `frames` rows\n",
    "        start_idx = (num_rows - frames) // 2\n",
    "        end_idx   = start_idx + frames\n",
    "        return non_zero_rows.iloc[start_idx:end_idx], True\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Consolidation loop\n",
    "# ------------------------------------------------------------\n",
    "all_dfs = []\n",
    "incomplete_files = {}\n",
    "\n",
    "for action, count in videos_dict_aug.items():\n",
    "    # Input CSV per sign (previous step already created these)\n",
    "    input_path = os.path.join(SYNTH_DATA_DIR, action, f\"{action}_lsm.csv\")\n",
    "\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"[MISSING] CSV not found for {action}: {input_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(input_path)\n",
    "    # Normalize headers just in case\n",
    "    df.columns = [col.upper().strip() for col in df.columns]\n",
    "\n",
    "    if \"VIDEO_SAMPLE\" not in df.columns:\n",
    "        print(f\"[ERR] 'VIDEO_SAMPLE' column not found in {input_path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Output per-sign CSV in the consolidated folder (optional)\n",
    "    output_file = os.path.join(TOTAL_DATA_DIR, f\"{action}_lsm.csv\")\n",
    "\n",
    "    incomplete_count = 0\n",
    "\n",
    "    for i in range(1, count + 1):\n",
    "        sample_data = df[df[\"VIDEO_SAMPLE\"] == i]\n",
    "\n",
    "        cleaned_data, is_complete = central_clean_data(sample_data, frames=30)\n",
    "\n",
    "        if cleaned_data.empty:\n",
    "            incomplete_count += 1\n",
    "            continue\n",
    "\n",
    "        # Append to global list\n",
    "        all_dfs.append(cleaned_data)\n",
    "\n",
    "        # Write per-sign file in append mode\n",
    "        # - first sample: write header\n",
    "        # - subsequent samples: append without header\n",
    "        write_mode = \"w\" if i == 1 else \"a\"\n",
    "        write_header = not os.path.exists(output_file) if i > 1 else True\n",
    "\n",
    "        cleaned_data.to_csv(\n",
    "            output_file,\n",
    "            mode=write_mode,\n",
    "            header=write_header,\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "        if not is_complete:\n",
    "            incomplete_count += 1\n",
    "\n",
    "    if incomplete_count > 0:\n",
    "        incomplete_files[action] = incomplete_count\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global consolidated CSV\n",
    "# ------------------------------------------------------------\n",
    "if all_dfs:\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    final_df.to_csv(FINAL_CSV_PATH, index=False)\n",
    "    print(f\"[OK] Consolidated CSV written to:\\n  {FINAL_CSV_PATH}\")\n",
    "else:\n",
    "    print(\"[WARN] No data collected for consolidation.\")\n",
    "\n",
    "# Summary of incomplete sequences\n",
    "if incomplete_files:\n",
    "    print(\"\\nSummary of sequences with fewer than 30 frames (after cleaning):\")\n",
    "    for action, count in incomplete_files.items():\n",
    "        print(f\"  {action}: {count} incomplete samples\")\n",
    "else:\n",
    "    print(\"\\nAll samples reached the minimum number of frames after cleaning.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aea0b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 230\n",
      "Column names: ['VIDEO_SAMPLE', 'CLASSIFICATION', 'FRAME', 'TIMESTAMP', 'RIGHT_WRIST_X', 'RIGHT_WRIST_Y', 'RIGHT_WRIST_Z', 'RIGHT_THUMB_CMC_X', 'RIGHT_THUMB_CMC_Y', 'RIGHT_THUMB_CMC_Z', 'RIGHT_THUMB_MCP_X', 'RIGHT_THUMB_MCP_Y', 'RIGHT_THUMB_MCP_Z', 'RIGHT_THUMB_IP_X', 'RIGHT_THUMB_IP_Y', 'RIGHT_THUMB_IP_Z', 'RIGHT_THUMB_TIP_X', 'RIGHT_THUMB_TIP_Y', 'RIGHT_THUMB_TIP_Z', 'RIGHT_INDEX_FINGER_MCP_X', 'RIGHT_INDEX_FINGER_MCP_Y', 'RIGHT_INDEX_FINGER_MCP_Z', 'RIGHT_INDEX_FINGER_PIP_X', 'RIGHT_INDEX_FINGER_PIP_Y', 'RIGHT_INDEX_FINGER_PIP_Z', 'RIGHT_INDEX_FINGER_DIP_X', 'RIGHT_INDEX_FINGER_DIP_Y', 'RIGHT_INDEX_FINGER_DIP_Z', 'RIGHT_INDEX_FINGER_TIP_X', 'RIGHT_INDEX_FINGER_TIP_Y', 'RIGHT_INDEX_FINGER_TIP_Z', 'RIGHT_MIDDLE_FINGER_MCP_X', 'RIGHT_MIDDLE_FINGER_MCP_Y', 'RIGHT_MIDDLE_FINGER_MCP_Z', 'RIGHT_MIDDLE_FINGER_PIP_X', 'RIGHT_MIDDLE_FINGER_PIP_Y', 'RIGHT_MIDDLE_FINGER_PIP_Z', 'RIGHT_MIDDLE_FINGER_DIP_X', 'RIGHT_MIDDLE_FINGER_DIP_Y', 'RIGHT_MIDDLE_FINGER_DIP_Z', 'RIGHT_MIDDLE_FINGER_TIP_X', 'RIGHT_MIDDLE_FINGER_TIP_Y', 'RIGHT_MIDDLE_FINGER_TIP_Z', 'RIGHT_RING_FINGER_MCP_X', 'RIGHT_RING_FINGER_MCP_Y', 'RIGHT_RING_FINGER_MCP_Z', 'RIGHT_RING_FINGER_PIP_X', 'RIGHT_RING_FINGER_PIP_Y', 'RIGHT_RING_FINGER_PIP_Z', 'RIGHT_RING_FINGER_DIP_X', 'RIGHT_RING_FINGER_DIP_Y', 'RIGHT_RING_FINGER_DIP_Z', 'RIGHT_RING_FINGER_TIP_X', 'RIGHT_RING_FINGER_TIP_Y', 'RIGHT_RING_FINGER_TIP_Z', 'RIGHT_PINKY_MCP_X', 'RIGHT_PINKY_MCP_Y', 'RIGHT_PINKY_MCP_Z', 'RIGHT_PINKY_PIP_X', 'RIGHT_PINKY_PIP_Y', 'RIGHT_PINKY_PIP_Z', 'RIGHT_PINKY_DIP_X', 'RIGHT_PINKY_DIP_Y', 'RIGHT_PINKY_DIP_Z', 'RIGHT_PINKY_TIP_X', 'RIGHT_PINKY_TIP_Y', 'RIGHT_PINKY_TIP_Z', 'LEFT_WRIST_X', 'LEFT_WRIST_Y', 'LEFT_WRIST_Z', 'LEFT_THUMB_CMC_X', 'LEFT_THUMB_CMC_Y', 'LEFT_THUMB_CMC_Z', 'LEFT_THUMB_MCP_X', 'LEFT_THUMB_MCP_Y', 'LEFT_THUMB_MCP_Z', 'LEFT_THUMB_IP_X', 'LEFT_THUMB_IP_Y', 'LEFT_THUMB_IP_Z', 'LEFT_THUMB_TIP_X', 'LEFT_THUMB_TIP_Y', 'LEFT_THUMB_TIP_Z', 'LEFT_INDEX_FINGER_MCP_X', 'LEFT_INDEX_FINGER_MCP_Y', 'LEFT_INDEX_FINGER_MCP_Z', 'LEFT_INDEX_FINGER_PIP_X', 'LEFT_INDEX_FINGER_PIP_Y', 'LEFT_INDEX_FINGER_PIP_Z', 'LEFT_INDEX_FINGER_DIP_X', 'LEFT_INDEX_FINGER_DIP_Y', 'LEFT_INDEX_FINGER_DIP_Z', 'LEFT_INDEX_FINGER_TIP_X', 'LEFT_INDEX_FINGER_TIP_Y', 'LEFT_INDEX_FINGER_TIP_Z', 'LEFT_MIDDLE_FINGER_MCP_X', 'LEFT_MIDDLE_FINGER_MCP_Y', 'LEFT_MIDDLE_FINGER_MCP_Z', 'LEFT_MIDDLE_FINGER_PIP_X', 'LEFT_MIDDLE_FINGER_PIP_Y', 'LEFT_MIDDLE_FINGER_PIP_Z', 'LEFT_MIDDLE_FINGER_DIP_X', 'LEFT_MIDDLE_FINGER_DIP_Y', 'LEFT_MIDDLE_FINGER_DIP_Z', 'LEFT_MIDDLE_FINGER_TIP_X', 'LEFT_MIDDLE_FINGER_TIP_Y', 'LEFT_MIDDLE_FINGER_TIP_Z', 'LEFT_RING_FINGER_MCP_X', 'LEFT_RING_FINGER_MCP_Y', 'LEFT_RING_FINGER_MCP_Z', 'LEFT_RING_FINGER_PIP_X', 'LEFT_RING_FINGER_PIP_Y', 'LEFT_RING_FINGER_PIP_Z', 'LEFT_RING_FINGER_DIP_X', 'LEFT_RING_FINGER_DIP_Y', 'LEFT_RING_FINGER_DIP_Z', 'LEFT_RING_FINGER_TIP_X', 'LEFT_RING_FINGER_TIP_Y', 'LEFT_RING_FINGER_TIP_Z', 'LEFT_PINKY_MCP_X', 'LEFT_PINKY_MCP_Y', 'LEFT_PINKY_MCP_Z', 'LEFT_PINKY_PIP_X', 'LEFT_PINKY_PIP_Y', 'LEFT_PINKY_PIP_Z', 'LEFT_PINKY_DIP_X', 'LEFT_PINKY_DIP_Y', 'LEFT_PINKY_DIP_Z', 'LEFT_PINKY_TIP_X', 'LEFT_PINKY_TIP_Y', 'LEFT_PINKY_TIP_Z', 'NOSE_X', 'NOSE_Y', 'NOSE_Z', 'NOSE_V', 'LEFT_EYE_INNER_X', 'LEFT_EYE_INNER_Y', 'LEFT_EYE_INNER_Z', 'LEFT_EYE_INNER_V', 'LEFT_EYE_X', 'LEFT_EYE_Y', 'LEFT_EYE_Z', 'LEFT_EYE_V', 'LEFT_EYE_OUTER_X', 'LEFT_EYE_OUTER_Y', 'LEFT_EYE_OUTER_Z', 'LEFT_EYE_OUTER_V', 'RIGHT_EYE_INNER_X', 'RIGHT_EYE_INNER_Y', 'RIGHT_EYE_INNER_Z', 'RIGHT_EYE_INNER_V', 'RIGHT_EYE_X', 'RIGHT_EYE_Y', 'RIGHT_EYE_Z', 'RIGHT_EYE_V', 'RIGHT_EYE_OUTER_X', 'RIGHT_EYE_OUTER_Y', 'RIGHT_EYE_OUTER_Z', 'RIGHT_EYE_OUTER_V', 'LEFT_EAR_X', 'LEFT_EAR_Y', 'LEFT_EAR_Z', 'LEFT_EAR_V', 'RIGHT_EAR_X', 'RIGHT_EAR_Y', 'RIGHT_EAR_Z', 'RIGHT_EAR_V', 'MOUTH_LEFT_X', 'MOUTH_LEFT_Y', 'MOUTH_LEFT_Z', 'MOUTH_LEFT_V', 'MOUTH_RIGHT_X', 'MOUTH_RIGHT_Y', 'MOUTH_RIGHT_Z', 'MOUTH_RIGHT_V', 'LEFT_SHOULDER_X', 'LEFT_SHOULDER_Y', 'LEFT_SHOULDER_Z', 'LEFT_SHOULDER_V', 'RIGHT_SHOULDER_X', 'RIGHT_SHOULDER_Y', 'RIGHT_SHOULDER_Z', 'RIGHT_SHOULDER_V', 'LEFT_ELBOW_X', 'LEFT_ELBOW_Y', 'LEFT_ELBOW_Z', 'LEFT_ELBOW_V', 'RIGHT_ELBOW_X', 'RIGHT_ELBOW_Y', 'RIGHT_ELBOW_Z', 'RIGHT_ELBOW_V', 'LEFT_WRIST_X.1', 'LEFT_WRIST_Y.1', 'LEFT_WRIST_Z.1', 'LEFT_WRIST_V', 'RIGHT_WRIST_X.1', 'RIGHT_WRIST_Y.1', 'RIGHT_WRIST_Z.1', 'RIGHT_WRIST_V', 'LEFT_PINKY_X', 'LEFT_PINKY_Y', 'LEFT_PINKY_Z', 'LEFT_PINKY_V', 'RIGHT_PINKY_X', 'RIGHT_PINKY_Y', 'RIGHT_PINKY_Z', 'RIGHT_PINKY_V', 'LEFT_INDEX_X', 'LEFT_INDEX_Y', 'LEFT_INDEX_Z', 'LEFT_INDEX_V', 'RIGHT_INDEX_X', 'RIGHT_INDEX_Y', 'RIGHT_INDEX_Z', 'RIGHT_INDEX_V', 'LEFT_THUMB_X', 'LEFT_THUMB_Y', 'LEFT_THUMB_Z', 'LEFT_THUMB_V', 'RIGHT_THUMB_X', 'RIGHT_THUMB_Y', 'RIGHT_THUMB_Z', 'RIGHT_THUMB_V', 'LEFT_HIP_X', 'LEFT_HIP_Y', 'LEFT_HIP_Z', 'LEFT_HIP_V', 'RIGHT_HIP_X', 'RIGHT_HIP_Y', 'RIGHT_HIP_Z', 'RIGHT_HIP_V']\n",
      "Total records: 2441\n",
      "\n",
      "Number of samples per classification:\n",
      "CLASSIFICATION\n",
      "ambulancia    11\n",
      "doctor        15\n",
      "dolor         20\n",
      "hoy           20\n",
      "yo            20\n",
      "Name: VIDEO_SAMPLE, dtype: int64\n",
      "[WARN] ambulancia has 11 samples; 20 were expected.\n",
      "[WARN] doctor has 15 samples; 20 were expected.\n",
      "\n",
      "Some samples have fewer than 30 frames:\n",
      "CLASSIFICATION  VIDEO_SAMPLE\n",
      "ambulancia      13              25\n",
      "                14              23\n",
      "doctor          11               4\n",
      "                13               3\n",
      "                7                1\n",
      "                9                6\n",
      "dolor           10              11\n",
      "                14              28\n",
      "dtype: int64\n",
      "Data for classification 'ambulancia':\n",
      "Total records: 318\n",
      "Number of columns: 230\n",
      "Unique samples: 11\n",
      "[WARN] Expected 20 samples, but found 11.\n",
      "\n",
      "Some samples have fewer than 30 frames:\n",
      "VIDEO_SAMPLE\n",
      "13    25\n",
      "14    23\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "BASE_DIR        = \"/Users/armandobecerril/PhD/MSL-150\"\n",
    "TOTAL_DATA_DIR  = os.path.join(BASE_DIR, \"data\", \"synthetic_sample_data_total\")\n",
    "FINAL_CSV_PATH  = os.path.join(TOTAL_DATA_DIR, \"MSL-150_Mexican_Sign_Language_Dataset.csv\")\n",
    "\n",
    "def analyze_csv_data(file_path, expected_samples, min_frames=30):\n",
    "    \"\"\"\n",
    "    Global sanity-check of the consolidated CSV:\n",
    "      - Prints general info (columns, rows).\n",
    "      - Checks number of VIDEO_SAMPLE per CLASSIFICATION.\n",
    "      - Verifies that each sample has at least `min_frames` rows.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Remove duplicated columns, if any\n",
    "    if df.columns.duplicated().any():\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "        print(\"Detected and removed duplicated column headers.\")\n",
    "\n",
    "    # Normalize types for key columns\n",
    "    df[\"CLASSIFICATION\"] = df[\"CLASSIFICATION\"].astype(str)\n",
    "    df[\"VIDEO_SAMPLE\"]   = df[\"VIDEO_SAMPLE\"].astype(str)\n",
    "\n",
    "    print(f\"Number of columns: {df.shape[1]}\")\n",
    "    print(f\"Column names: {list(df.columns)}\")\n",
    "    print(f\"Total records: {df.shape[0]}\")\n",
    "\n",
    "    if \"CLASSIFICATION\" in df.columns and \"VIDEO_SAMPLE\" in df.columns:\n",
    "        sample_count = df.groupby(\"CLASSIFICATION\")[\"VIDEO_SAMPLE\"].nunique()\n",
    "        print(\"\\nNumber of samples per classification:\")\n",
    "        print(sample_count)\n",
    "\n",
    "        # Check expected_samples per class\n",
    "        for classification, count in sample_count.items():\n",
    "            if count != expected_samples:\n",
    "                print(\n",
    "                    f\"[WARN] {classification} has {count} samples; \"\n",
    "                    f\"{expected_samples} were expected.\"\n",
    "                )\n",
    "\n",
    "        # Check frame count per (classification, video_sample)\n",
    "        frame_sizes = df.groupby([\"CLASSIFICATION\", \"VIDEO_SAMPLE\"]).size()\n",
    "        frame_issues = frame_sizes[frame_sizes < min_frames]\n",
    "\n",
    "        if not frame_issues.empty:\n",
    "            print(f\"\\nSome samples have fewer than {min_frames} frames:\")\n",
    "            print(frame_issues)\n",
    "        else:\n",
    "            print(f\"\\nAll samples have at least {min_frames} frames.\")\n",
    "    else:\n",
    "        print(\"Missing 'CLASSIFICATION' or 'VIDEO_SAMPLE' columns in the DataFrame.\")\n",
    "\n",
    "\n",
    "def deep_validate_classification(\n",
    "    file_path,\n",
    "    classification,\n",
    "    expected_samples,\n",
    "    min_frames=30\n",
    "):\n",
    "    \"\"\"\n",
    "    Detailed inspection for a single classification:\n",
    "      - Number of records and columns.\n",
    "      - Number of unique VIDEO_SAMPLE values.\n",
    "      - Frame count per sample, with a check for min_frames.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    class_df : pd.DataFrame\n",
    "        Subset of the data for the given classification.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    df[\"CLASSIFICATION\"] = df[\"CLASSIFICATION\"].astype(str)\n",
    "    df[\"VIDEO_SAMPLE\"]   = df[\"VIDEO_SAMPLE\"].astype(str)\n",
    "\n",
    "    class_df = df[df[\"CLASSIFICATION\"] == str(classification)]\n",
    "\n",
    "    print(f\"Data for classification '{classification}':\")\n",
    "    print(f\"Total records: {class_df.shape[0]}\")\n",
    "    print(f\"Number of columns: {class_df.shape[1]}\")\n",
    "\n",
    "    unique_samples = class_df[\"VIDEO_SAMPLE\"].nunique()\n",
    "    print(f\"Unique samples: {unique_samples}\")\n",
    "\n",
    "    if unique_samples != expected_samples:\n",
    "        print(\n",
    "            f\"[WARN] Expected {expected_samples} samples, \"\n",
    "            f\"but found {unique_samples}.\"\n",
    "        )\n",
    "\n",
    "    sample_frames = class_df.groupby(\"VIDEO_SAMPLE\").size()\n",
    "    frame_issues = sample_frames[sample_frames < min_frames]\n",
    "\n",
    "    if not frame_issues.empty:\n",
    "        print(f\"\\nSome samples have fewer than {min_frames} frames:\")\n",
    "        print(frame_issues)\n",
    "    else:\n",
    "        print(f\"\\nAll samples have at least {min_frames} frames.\")\n",
    "\n",
    "    return class_df\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Run the checks for the consolidated “sample” dataset\n",
    "# ------------------------------------------------------------\n",
    "# For your current mini-dataset, expected_samples should match `samples`\n",
    "# (e.g., 20 synthetic videos per sign).\n",
    "analyze_csv_data(FINAL_CSV_PATH, expected_samples=samples, min_frames=30)\n",
    "\n",
    "# Example: deep inspection for one sign (e.g. \"AMBULANCIA\")\n",
    "df_ambulancia = deep_validate_classification(\n",
    "    FINAL_CSV_PATH,\n",
    "    classification=\"ambulancia\",\n",
    "    expected_samples=samples,\n",
    "    min_frames=30\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c3cb00",
   "metadata": {},
   "source": [
    "# 6. Data Preparation Directories & Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906d93ab",
   "metadata": {},
   "source": [
    "Extraer keypoints desde el CSV consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f8398e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BASE_DIR        = \"/Users/armandobecerril/PhD/MSL-150\"\n",
    "TOTAL_DATA_DIR  = os.path.join(BASE_DIR, \"data\", \"synthetic_sample_data_total\")\n",
    "FINAL_CSV_PATH  = os.path.join(TOTAL_DATA_DIR, \"MSL-150_Mexican_Sign_Language_Dataset.csv\")\n",
    "\n",
    "SAMPLE_NPY_ROOT = os.path.join(BASE_DIR, \"data\", \"sample_npy\")\n",
    "os.makedirs(SAMPLE_NPY_ROOT, exist_ok=True)\n",
    "\n",
    "def extract_keypoints_lsm_from_csv(df, classification, video_sample_num):\n",
    "    \"\"\"\n",
    "    Extrae los keypoints de mano derecha, mano izquierda y pose\n",
    "    desde el DataFrame consolidado con layout:\n",
    "\n",
    "    VIDEO_SAMPLE, CLASSIFICATION, FRAME, TIMESTAMP,\n",
    "    RIGHT_*, LEFT_*, [POSE_* ...]\n",
    "\n",
    "    Asume índices:\n",
    "      - RIGHT hand: columnas 4–66   (63 columnas = 21×3)\n",
    "      - LEFT hand : columnas 67–129 (63 columnas = 21×3)\n",
    "      - POSE      : columnas 130–229 (100 columnas = 25×4)\n",
    "    \"\"\"\n",
    "    # Normalizar encabezados\n",
    "    df.columns = [c.strip().upper() for c in df.columns]\n",
    "\n",
    "    # Filtrar por clasificación y sample\n",
    "    filtered_data = df[\n",
    "        (df[\"CLASSIFICATION\"] == classification) &\n",
    "        (df[\"VIDEO_SAMPLE\"] == video_sample_num)\n",
    "    ]\n",
    "\n",
    "    if filtered_data.empty:\n",
    "        pose_data = np.zeros((1, 100))  # 25×4\n",
    "        lh_data   = np.zeros((1, 63))   # 21×3\n",
    "        rh_data   = np.zeros((1, 63))   # 21×3\n",
    "    else:\n",
    "        rh_data   = filtered_data.iloc[:, 4:67].values    # 63 cols\n",
    "        lh_data   = filtered_data.iloc[:, 67:130].values  # 63 cols\n",
    "        pose_data = filtered_data.iloc[:, 130:230].values # 100 cols\n",
    "\n",
    "    # Concatenar: [POSE | LH | RH] como en tu pipeline original\n",
    "    return np.concatenate([pose_data, lh_data, rh_data], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa7826",
   "metadata": {},
   "source": [
    "Guardar .npy asegurando exactamente 30 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aee888e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sample_to_npy(classification, video_sample, keypoints_data, output_base_path, target_frames=30):\n",
    "    \"\"\"\n",
    "    Guarda los keypoints de un sample en:\n",
    "\n",
    "        output_base_path/<CLASSIFICATION>/<VIDEO_SAMPLE>/<0..29>.npy\n",
    "\n",
    "    Forzando exactamente `target_frames` frames por muestra.\n",
    "    \"\"\"\n",
    "    num_frames, num_features = keypoints_data.shape\n",
    "\n",
    "    if num_frames == 0:\n",
    "        print(f\"[WARN] Sample {classification}-{video_sample} no tiene frames válidos. Saltando.\")\n",
    "        return\n",
    "\n",
    "    # Si hay más de target_frames, volvemos a centrar (por seguridad)\n",
    "    if num_frames > target_frames:\n",
    "        start = (num_frames - target_frames) // 2\n",
    "        keypoints_data = keypoints_data[start:start + target_frames]\n",
    "        num_frames = target_frames\n",
    "\n",
    "    # Si hay menos, rellenamos con ceros hasta target_frames\n",
    "    if num_frames < target_frames:\n",
    "        pad = np.zeros((target_frames - num_frames, num_features))\n",
    "        keypoints_data = np.vstack([keypoints_data, pad])\n",
    "        num_frames = target_frames\n",
    "\n",
    "    # Crear directorio: data/sample_npy/<CLASS>/<VIDEO_SAMPLE>/\n",
    "    sample_dir = os.path.join(output_base_path, str(classification), str(video_sample))\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "    for idx in range(num_frames):\n",
    "        np.save(os.path.join(sample_dir, f\"{idx}.npy\"), keypoints_data[idx])\n",
    "\n",
    "    print(f\"[OK] {classification} - sample {video_sample}: guardados {num_frames} npy en {sample_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c3923",
   "metadata": {},
   "source": [
    "Orquestador: del CSV consolidado npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fc47ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Clasificaciones encontradas en el CSV sample: ['ambulancia', 'doctor', 'dolor', 'hoy', 'yo']\n",
      "\n",
      "[INFO] Procesando clasificación 'ambulancia' con 11 samples\n",
      "[OK] ambulancia - sample 1: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/ambulancia/1\n",
      "[OK] ambulancia - sample 3: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/ambulancia/3\n",
      "[OK] ambulancia - sample 4: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/ambulancia/4\n",
      "[OK] ambulancia - sample 6: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/ambulancia/6\n",
      "[OK] ambulancia - sample 7: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/ambulancia/7\n",
      "[OK] ambulancia - sample 8: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/ambulancia/8\n",
      "[OK] ambulancia - sample 12: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/ambulancia/12\n",
      "[OK] ambulancia - sample 13: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/ambulancia/13\n",
      "[OK] ambulancia - sample 14: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/ambulancia/14\n",
      "[OK] ambulancia - sample 15: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/ambulancia/15\n",
      "[OK] ambulancia - sample 18: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/ambulancia/18\n",
      "\n",
      "[INFO] Procesando clasificación 'doctor' con 15 samples\n",
      "[OK] doctor - sample 2: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/doctor/2\n",
      "[OK] doctor - sample 3: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/doctor/3\n",
      "[OK] doctor - sample 4: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/doctor/4\n",
      "[OK] doctor - sample 6: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/doctor/6\n",
      "[OK] doctor - sample 7: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/doctor/7\n",
      "[OK] doctor - sample 9: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/doctor/9\n",
      "[OK] doctor - sample 10: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/doctor/10\n",
      "[OK] doctor - sample 11: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/doctor/11\n",
      "[OK] doctor - sample 12: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/doctor/12\n",
      "[OK] doctor - sample 13: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/doctor/13\n",
      "[OK] doctor - sample 15: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/doctor/15\n",
      "[OK] doctor - sample 16: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/doctor/16\n",
      "[OK] doctor - sample 18: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/doctor/18\n",
      "[OK] doctor - sample 19: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/doctor/19\n",
      "[OK] doctor - sample 20: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/doctor/20\n",
      "\n",
      "[INFO] Procesando clasificación 'dolor' con 20 samples\n",
      "[OK] dolor - sample 1: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/1\n",
      "[OK] dolor - sample 2: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/2\n",
      "[OK] dolor - sample 3: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/3\n",
      "[OK] dolor - sample 4: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/4\n",
      "[OK] dolor - sample 5: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/5\n",
      "[OK] dolor - sample 6: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/6\n",
      "[OK] dolor - sample 7: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/7\n",
      "[OK] dolor - sample 8: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/8\n",
      "[OK] dolor - sample 9: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/9\n",
      "[OK] dolor - sample 10: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/10\n",
      "[OK] dolor - sample 11: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/11\n",
      "[OK] dolor - sample 12: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/12\n",
      "[OK] dolor - sample 13: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/13\n",
      "[OK] dolor - sample 14: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/14\n",
      "[OK] dolor - sample 15: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/15\n",
      "[OK] dolor - sample 16: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/16\n",
      "[OK] dolor - sample 17: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/17\n",
      "[OK] dolor - sample 18: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/18\n",
      "[OK] dolor - sample 19: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/19\n",
      "[OK] dolor - sample 20: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/dolor/20\n",
      "\n",
      "[INFO] Procesando clasificación 'hoy' con 20 samples\n",
      "[OK] hoy - sample 1: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/1\n",
      "[OK] hoy - sample 2: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/2\n",
      "[OK] hoy - sample 3: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/3\n",
      "[OK] hoy - sample 4: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/4\n",
      "[OK] hoy - sample 5: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/5\n",
      "[OK] hoy - sample 6: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/6\n",
      "[OK] hoy - sample 7: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/7\n",
      "[OK] hoy - sample 8: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/8\n",
      "[OK] hoy - sample 9: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/9\n",
      "[OK] hoy - sample 10: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/10\n",
      "[OK] hoy - sample 11: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/11\n",
      "[OK] hoy - sample 12: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/12\n",
      "[OK] hoy - sample 13: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/13\n",
      "[OK] hoy - sample 14: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/14\n",
      "[OK] hoy - sample 15: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/15\n",
      "[OK] hoy - sample 16: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/16\n",
      "[OK] hoy - sample 17: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/17\n",
      "[OK] hoy - sample 18: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/18\n",
      "[OK] hoy - sample 19: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/19\n",
      "[OK] hoy - sample 20: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/hoy/20\n",
      "\n",
      "[INFO] Procesando clasificación 'yo' con 20 samples\n",
      "[OK] yo - sample 1: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/1\n",
      "[OK] yo - sample 2: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/2\n",
      "[OK] yo - sample 3: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/3\n",
      "[OK] yo - sample 4: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/4\n",
      "[OK] yo - sample 5: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/5\n",
      "[OK] yo - sample 6: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/6\n",
      "[OK] yo - sample 7: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/7\n",
      "[OK] yo - sample 8: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/8\n",
      "[OK] yo - sample 9: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/9\n",
      "[OK] yo - sample 10: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/10\n",
      "[OK] yo - sample 11: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/11\n",
      "[OK] yo - sample 12: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/12\n",
      "[OK] yo - sample 13: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/13\n",
      "[OK] yo - sample 14: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/14\n",
      "[OK] yo - sample 15: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/15\n",
      "[OK] yo - sample 16: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/16\n",
      "[OK] yo - sample 17: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/17\n",
      "[OK] yo - sample 18: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/18\n",
      "[OK] yo - sample 19: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/19\n",
      "[OK] yo - sample 20: guardados 30 npy en /Users/armandobecerril/PhD/MSL-150/data/sample_npy/yo/20\n",
      "\n",
      "[DONE] Todos los samples convertidos a npy en: /Users/armandobecerril/PhD/MSL-150/data/sample_npy\n"
     ]
    }
   ],
   "source": [
    "def process_consolidated_csv_to_npy(consolidated_csv_path, npy_root_dir, target_frames=30):\n",
    "    \"\"\"\n",
    "    Lee MSL-150_Mexican_Sign_Language_Dataset.csv (versión sample)\n",
    "    y genera data/sample_npy/<CLASSIFICATION>/<VIDEO_SAMPLE>/0..29.npy\n",
    "    \"\"\"\n",
    "    if not os.path.exists(consolidated_csv_path):\n",
    "        raise FileNotFoundError(f\"Consolidated CSV not found: {consolidated_csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(consolidated_csv_path)\n",
    "    df.columns = [c.strip().upper() for c in df.columns]\n",
    "\n",
    "    if \"CLASSIFICATION\" not in df.columns or \"VIDEO_SAMPLE\" not in df.columns:\n",
    "        raise ValueError(\"El CSV consolidado debe contener columnas 'CLASSIFICATION' y 'VIDEO_SAMPLE'.\")\n",
    "\n",
    "    classes = sorted(df[\"CLASSIFICATION\"].unique())\n",
    "    print(f\"[INFO] Clasificaciones encontradas en el CSV sample: {classes}\")\n",
    "\n",
    "    for classification in classes:\n",
    "        df_class = df[df[\"CLASSIFICATION\"] == classification]\n",
    "        samples = sorted(df_class[\"VIDEO_SAMPLE\"].unique())\n",
    "        print(f\"\\n[INFO] Procesando clasificación '{classification}' con {len(samples)} samples\")\n",
    "\n",
    "        for video_sample in samples:\n",
    "            keypoints_data = extract_keypoints_lsm_from_csv(\n",
    "                df,\n",
    "                classification=classification,\n",
    "                video_sample_num=video_sample\n",
    "            )\n",
    "            save_sample_to_npy(\n",
    "                classification=classification,\n",
    "                video_sample=video_sample,\n",
    "                keypoints_data=keypoints_data,\n",
    "                output_base_path=npy_root_dir,\n",
    "                target_frames=target_frames\n",
    "            )\n",
    "\n",
    "    print(f\"\\n[DONE] Todos los samples convertidos a npy en: {npy_root_dir}\")\n",
    "\n",
    "\n",
    "# Llamada principal\n",
    "process_consolidated_csv_to_npy(FINAL_CSV_PATH, SAMPLE_NPY_ROOT, target_frames=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f89a1",
   "metadata": {},
   "source": [
    "To ensure compatibility with the recurrent architecture, all samples must share the same temporal and spatial dimensionality. The model was trained on fixed-length sequences of 30 frames × 226 features, and therefore cannot process variable-length inputs during inference. To guarantee consistency and avoid distributional drift, each sequence is temporally standardized using a centered 30-frame window. Sequences shorter than 30 frames are zero-padded, while longer sequences are symmetrically trimmed. This ensures stable recurrent dynamics and reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23daf6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
